{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Zero or One? (100 marks)\n",
    "\n",
    "All you will be given about this problem is a training data set. Your objective is to develop a classifier that will have the highest accuracy in unseen examples.\n",
    "\n",
    "The following cell loads the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training data set: (5000, 39)\n",
      "[[0. 1. 1. ... 0. 0. 0.]\n",
      " [1. 0. 1. ... 0. 1. 0.]\n",
      " [1. 1. 1. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 1. 0.]\n",
      " [1. 0. 1. ... 0. 1. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sources\\n\\n    https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python\\n        Accessed 19:00, 02/05/2019\\n        \\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_data = np.loadtxt(open(\"data/training_data.csv\"), delimiter=\",\")\n",
    "print(\"Shape of the training data set:\", training_data.shape)\n",
    "print(training_data)\n",
    "\n",
    "\n",
    "import sklearn # Load in some modules I want (help for data manipulation, calculating values from data & a logstic regressions library)\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "'''Sources\n",
    "\n",
    "    https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python\n",
    "        Accessed 19:00, 02/05/2019\n",
    "        \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is again the response variable. The remaining 38 columns are binary features. You have multiple tasks:\n",
    "\n",
    "(1) Your first task is to write a function called `train()` that takes `training_data` as input and returns all the fitted parameters of your model. Note that the fitted parameters of your model depend on the model you choose. For example, if you use a na√Øve Bayes classifier, you could return a list of class priors and conditional likelihoods. (This function will allow us to compute your model on the fly. We should be able to execute it in less than 10 minutes.) \n",
    "\n",
    "(2) Your second task is to provide a variable called `fitted_model` which stores the model parameters you found by executing your train() function on the training_data. If your train function takes more than 20 seconds to run, this variable should load precomputed parameter values (possibly from a file) rather than execute the train() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "98502b22b7569c3888bbea2dae89e8a6",
     "grade": false,
     "grade_id": "cell-9284ab8e9721ffc2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def train(training_data):\n",
    "    \"\"\"\n",
    "    Train a model on the training_data\n",
    "\n",
    "    :param training_data: a two-dimensional numpy-array with shape = [5000, 39] \n",
    "    \n",
    "    :return fitted_model: any data structure that captures your model\n",
    "    \"\"\"\n",
    "    response = training_data[:,0] # Neatly defining\n",
    "    features = training_data[:,1:(training_data.shape[1])]\n",
    "    \n",
    "    logRegClass = LogisticRegression() #Our logistic regression object will contained our fitted model, so this is what we\n",
    "                                        # will return\n",
    "        \n",
    "    logRegClass.fit(features, response) #Train our model using the logistic regression library\n",
    "    \n",
    "    fitted_model = logRegClass\n",
    "    \n",
    "    \n",
    "    return fitted_model\n",
    "\n",
    "## Uncomment one of the following two lines depending on whether you want us to compute your model on the \n",
    "## fly or load a supplementary file.\n",
    "\n",
    "fitted_model = train(training_data)\n",
    "# fitted_model = load(local_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Your third task is to provide a function called `test()` that uses your `fitted_model` to classify the observations of `testing_data`. The `testing_data` is hidden and may contain any number of observations (rows). It contains 38 columns that have the same structure as the features of `training_data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8547b4673b0548ddd786722f367268a4",
     "grade": false,
     "grade_id": "cell-b62974b058e95e23",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def test(testing_data, fitted_model):\n",
    "    \"\"\"\n",
    "    Classify the rows of testing_data using a fitted_model. \n",
    "\n",
    "    :param testing_data: a two-dimensional numpy-array with shape = [n_test_samples, 38]\n",
    "    :param fitted_model: the output of your train function.\n",
    "\n",
    "    :return class_predictions: a numpy array containing the class predictions for each row\n",
    "        of testing_data.\n",
    "    \"\"\"\n",
    "    features = testing_data\n",
    "    \n",
    "    logRegClass = fitted_model # Fitted model is a logistic regression object from the logistic regression library\n",
    "    responsePrediction = logRegClass.predict(features)\n",
    "    \n",
    "    class_predictions = responsePrediction\n",
    "    \n",
    "    return class_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response = training_data[:,0] # Neatly defining\n",
    "#features = training_data[:,1:(training_data.shape[1])]\n",
    "#\n",
    "#rows = 250\n",
    "#class_predictions = test(training_data[:rows, 1:], fitted_model)\n",
    "#\n",
    "#print(\"Accuracy:\",metrics.accuracy_score(class_predictions, response[:rows]))\n",
    "#print(\"Precision:\",metrics.precision_score(class_predictions, response[:rows]))\n",
    "#print(\"Recall:\",metrics.recall_score(class_predictions, response[:rows]))\n",
    "#\n",
    "#responsePrediction_curve = fitted_model.predict_proba(features)[::,1] # ROC Curve\n",
    "#fpr, tpr, _ = metrics.roc_curve(response, responsePrediction_curve)\n",
    "#auc = metrics.roc_auc_score(response, responsePrediction_curve)\n",
    "#plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "#plt.legend(loc=4)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8e632be3b415f26b688aaf8da8354865",
     "grade": true,
     "grade_id": "cell-d6503e588628ad10",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell. Do not delete or change. \n",
    "# You can use this cell to check whether the returned objects of your function are of the right data type.\n",
    "\n",
    "# Test data types if input are the first 20 rows of the training_data.\n",
    "class_predictions = test(training_data[:20, 1:], fitted_model)\n",
    "\n",
    "# Check data type(s)\n",
    "assert(isinstance(class_predictions, np.ndarray))\n",
    "\n",
    "# Check shape of numpy array\n",
    "assert(class_predictions.shape == (20,))\n",
    "\n",
    "# Check data type of array elements\n",
    "assert(np.all(np.logical_or(class_predictions == 0, class_predictions == 1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7dcae32826a9c82b4897eec6dc9470b6",
     "grade": true,
     "grade_id": "cell-1d336f7ffecd8f71",
     "locked": false,
     "points": 100,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Describe in less than 10 sentences: Explain your classifier. Comment on its performance. What other alternative classifiers did you consider or experiment with? How does the performance of your classifier change as the size of the training set increases? You may want to include figures. \n",
    "---------\n",
    "   I've used a Logistic Regression classifier and been able to implement this with ease through the **LogisticRegression** class from the **sklearn** library - this comes from the logistic function, but uses $beta_i$ and $x_i$ to predict the probability an entry has response 0 or 1 ($beta_i$ are generated through the train function).\n",
    "   \n",
    "   To look at the perfomance of the fitted model I looked at *accuracy*, *precision* and *recall* - I was able to get these values from *training_data* by training on 4750 rows of data and testing on 250 rows of data. Accuracy is self explanatory - how many the fitted model got correct - precision is how correct the fitted model is - what's the chance that when it makes a prediction, that prediction is correct - and recall denotes how useful the model is - in the example of spam/ham emails, the recall denotes how many spam emails are correctly identified. When running this test I got Accuracy=93.2%, Precision=96.1%, Recall=91.1% - which is very good!\n",
    "   \n",
    "   I also took a look at an ROC graph (Receiver Operating Characteristic). It shows how well the predictive model can predict the true response values, and the tradeoff between *sensitivity* and *specificity* - The ROC curve does this by plotting **sensitivity**, the probability of predicting a real positive will be a positive, against **1-specificity** the probability of predicting a real negative will be a positive. AUC score for my test was 0.985 - an AUC score 1 represents perfect classifier, and 0.5 represents a worthless classifier.\n",
    "   \n",
    "   I considered using Naive Bayes, but I wanted to try a different classifier and I didn't get it working as well as I could have in coursework 3 so I wanted to see if I could improve with something else. Could've used k-nearest neighbour, but there is 38 different features and it may have been difficult to find the optimum k.\n",
    "   \n",
    "   Even with 5000 rows, it takes less than 1 second for the classifier to train and test.\n",
    "   <img src=\"images/my_graphics.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
