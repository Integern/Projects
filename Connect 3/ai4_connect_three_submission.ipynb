{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Three \n",
    "\n",
    "The primary description of this coursework is available on the CM20252 Moodle page. This is the Jupyter notebook you must complete and submit to receive marks. This notebook adds additional detail to the coursework specification but does not repeat the information that has already been provided there. \n",
    "\n",
    "You must follow all instructions given in this notebook precisely.\n",
    "\n",
    "Restart the kernel and run all cells before submitting the notebook. This will guarantee that we will be able to run your code for testing. Remember to save your work regularly.\n",
    "\n",
    "__You will develop players for Connect-Three on a grid that is 5 columns wide and 3 rows high. An example is shown below showing a win for Player Red.__\n",
    "\n",
    "<img src=\"images/connect3.png\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "For your reference, below is a visual depiction of the agent-environment interface in reinforcement learning. The interaction of the agent with its environments starts at decision stage $t=0$ with the observation of the current state $s_0$. (Notice that there is no reward at this initial stage.) The agent then chooses an action to execute at decision stage $t=1$. The environment responds by changing its state to $s_1$ and returning the numerical reward signal $r_1$. \n",
    "\n",
    "<img src=\"images/agent-environment.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "Below, we provide some code that will be useful for implementing parts of this interface. You are not obligated to use this code; please feel free to develop your own code from scratch. \n",
    "\n",
    "### Code details\n",
    "\n",
    "We provide a `Connect` class that you can use to simulate Connect-Three games. The following cells in this section will walk you through the basic usage of this class by playing a couple of games.\n",
    "\n",
    "We import the `connect` module and create a Connect-Three environment called `env`. The constructor method has one argument called `verbose`. If `verbose=True`, the `Connect` object will regularly print the progress of the game. This is useful for getting to know the provided code, debugging your code, or if you just want to play around. You will want to set `verbose=False` when you run hundreds of episodes to complete the marked exercises.\n",
    "\n",
    "This `Connect` environment uses the strings `'o'` and `'x'` instead of different disk colors in order to distinguish between the two players. We can specify who should start the game using the `starting_player` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game has been reset.\n",
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "import connect\n",
    "env = connect.Connect(starting_player='x', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can interact with the environment using the `act()` method. This method takes an `action` (an integer) as input and computes the response of the environment. An action is defined as the column index that a disk is dropped into. The `act()` method returns the `reward` for player `'o'` and a boolean, indicating whether the game is over (`True`) or not (`False`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']]\n",
      "reward = 0\n",
      "game_over = False\n"
     ]
    }
   ],
   "source": [
    "reward, game_over = env.act(action=2)\n",
    "print(\"reward =\", reward)\n",
    "print(\"game_over =\", game_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we set `verbose=True` when we created our environment, the grid is printed each time we call the `act()` method. You probably might want to set `verbose=False` when you run Q-learning for thousands of episodes. \n",
    "\n",
    "As expected, the `reward` is 0 and no one has won the game yet (`game_over` is `False`). Let us drop another disk into the same column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "reward, game_over = env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the `Connect` environment automatically switches the\n",
    "\n",
    "The `grid` is stored as a two-dimensional `numpy` array in the `Connect` class and you can easily access it by calling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' 'x' ' ' ' ']\n",
      " [' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "current_grid = env.grid\n",
    "print(current_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the grid now appears to be \"upside down\" because `numpy` arrays are printed from \"top to bottom\".\n",
    "We can also print it the way it is printed by the Connect class by calling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "print(current_grid[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make another move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' 'x' ' ' ' ']\n",
      " [' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "reward, game_over = env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to put another disk in the same column with `act(action=2)`. The environment will throw an error because that column is already filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-08cd2c81a094>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# This cell should throw an IndexError!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\connect.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \"\"\"\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lowest_free_row_per_column\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactive_player\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lowest_free_row_per_column\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m  \u001b[1;31m# You can ignore this; internal use only.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lowest_free_row_per_column\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_rows\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "# This cell should throw an IndexError!\n",
    "env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute `.available_actions` of the `Connect` class contains a `numpy` array of all not yet filled columns. This variable should help you to avoid errors like the one we have just encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.available_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that column index '2' is missing because this column is already filled.\n",
    "\n",
    "Let's keep on playing until some player wins..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward, game_over = env.act(action=3)\n",
    "print(\"reward =\", reward, \"game_over =\", game_over) \n",
    "reward, game_over = env.act(action=1)\n",
    "print(\"reward =\", reward, \"game_over =\", game_over)\n",
    "reward, game_over = env.act(action=3)\n",
    "print(\"reward =\", reward, \"game_over =\", game_over)\n",
    "reward, game_over = env.act(action=1)\n",
    "print(\"reward =\", reward, \"game_over =\", game_over)\n",
    "reward, game_over = env.act(action=3)\n",
    "print(\"reward =\", reward, \"game_over =\", game_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the `reward` returned by the `act()` method is the reward for player `'o'`.\n",
    "\n",
    "You can reset the game using the `reset()` method. This method cleans the grid and makes sure that the it is the `starting_player`'s turn as defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "reward, game_over = env.act(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to modify existing or add new methods to the `Connect` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "**Your opponent is always the first player. Your agent is always the second player.**\n",
    "\n",
    "For your reference, the pseudo-code for Q-learning is reproduced below from the textbook (Reinforcement Learning, Sutton & Barto, 1998, Section 6.5).\n",
    "<img src=\"images/q_learning.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "Prepare a **learning curve** following the directions below. We refer to this as Plot 1.\n",
    "\n",
    "After $n$ steps of interaction with the environment, play $m$ games with the current policy of the agent (without modifying the policy). Think of this as interrupting the agent for a period of time to test how well it has learned so far. Your plot should show the total score obtained in these $m$ games as a function of $n, 2n, 3n, … kn$. The choices of $n$ and $k$ are up to you. They should be reasonable values that demonstrate the efficiency of the learning and how well the agent learns to play the game eventually. Use $m=10$. \n",
    "\n",
    "This plot should show the mean performance of `a` agents, not the performance of a single agent. Because of the stochasticity in the environment, you will obtain two different learning curves from two different agents even though they are using exactly the same algorithm. We suggest setting `a` to 20 or higher.\n",
    "\n",
    "Present a single mean learning curve with your choice of parameters $\\epsilon$ and $\\alpha$. The plot should also show (as a baseline) the mean performance of a random agent that does not learn but chooses actions uniformly randomly from among the legal actions. Label this line “Random Agent”. \n",
    "\n",
    "Please include this plot as a static figure in the appropriate cell below. That is, compute the learning curve in the lab or at home (this may take a couple of minutes depending on your implementation) and save the figure in the same directory as your notebook. Import this figure in the appropriate answer cell under (A). You can look at the source code of this markdown cell (double click on it!) to find out how to embed figures using html. Do **not** use drag & drop to include figures; we would not be able to see them! Make sure to include the locally stored images in your submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1431aa87b9e9019a4dbe6e696e0a9082",
     "grade": true,
     "grade_id": "cell-3ac2114f764e8410",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 1000\n",
    "k = 20\n",
    "m = 10\n",
    "\n",
    "a = 5\n",
    "\n",
    "scores = np.zeros( (a, k) )\n",
    "for A in range(a): #Each Agent\n",
    "    \n",
    "    print('Agent ', A)\n",
    "    game = qLearning()\n",
    "    for K in range(k): #Learning Stages\n",
    "        #game.learn(n)\n",
    "        \n",
    "        _, score = game.rand_game(m)\n",
    "        scores[A][K] = score\n",
    "        print('Score: ', score, ' (', K, '/', k,')')\n",
    "        \n",
    "averageScores = np.zeros(k)\n",
    "for C in range(k):\n",
    "    total = 0\n",
    "    for R in range(a):\n",
    "        total = total + scores[R][C]\n",
    "        \n",
    "    averageScores[C] = total / a\n",
    "    \n",
    "print(scores)\n",
    "print('')\n",
    "print(averageScores)\n",
    "\n",
    "plt.plot(averageScores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('# of games learnt from')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "980d73fb62fae59d610abc96121f71bc",
     "grade": true,
     "grade_id": "cell-ce1405b859519f91",
     "locked": false,
     "points": 60,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "(A) [continued} Insert your static learning curve here (Plot 1).\n",
    "\n",
    "<img src=\"images/20_agents.png\" style=\"width: 600px;\"/>\n",
    "<img src=\"images/20_agents_overlap.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "For these graphs, I had the following values:\n",
    "-20 Agents\n",
    "-n = 1000\n",
    "-k = 20\n",
    "-m = 10\n",
    "-epsilon = .7\n",
    "-alpha = .5\n",
    "-lambda = .7 (discount factor)\n",
    "\n",
    "\n",
    "(B) In 3 sentences or less, explain your conclusions from the plot above. How close does your (average) agent get to the best possible level of performance? How efficiently does your (average) agent learn? \n",
    "\n",
    "From the plots above it is clear that training the AI improves how well it performs. The best possible level of performance would be a score of 10 by winning each time - here it is clear that by the end it doesn't reach this level but gets 4 more wins (on average) than the opponent does, which equates to a win rate of roughly 70%. After researching on Q-Learning, I saw a good amount of games to train an AI on is 100,000 games, here it gets to 70% winrate with just 20,000 - it doesn't need a lot of games, but it does take a while computationally (had to wait around 30 minutes to do a total of 20*20,000 = 400,000 games).\n",
    "\n",
    "\n",
    "(C) In five sentences or less, explain the key aspects of your implementation. How many state-action pairs do you represent in your Q-table? Describe and justify your settings of $\\alpha$ and $\\epsilon$. Are there any things you tried out that are not in your final implementation?\n",
    "\n",
    "The basics of my implementation was having a game class and a qLearning class - the game class hooks into connect.py and gives me useful functions like having the opponent move in a random, valid position. The qLearning class was a lot more detailed - I have an epsilon-greedy function which helps us choose which position for the agent to move, functions that make it easy to update my Q-Table etc.. \n",
    "I set up my Q-table using a dictionary instead of a fixed sized array - there were going to be a lot of states that would be impossible to reach (table full of x) and some states that would rarely be reached. For each state, it had 5 possible actions. Obviously some states wouldn't be able to execute all 5 though.\n",
    "I set my alpha to 0.5 - for the implementation of this game, a terminal state is reachable very quickly as the opponent has to strategy to their movements so doesn't need a big memory, but winning is not instantanous. Epsilon is set to 0.7, I found this a good value to have the right balance of moving randomly to get the Q-Table updated with a variety of values as well as to find an optimum solution.\n",
    "One thing I didn't keep in my implementation (due to the specification of the coursework) was varying the epsilon value. At the beginning when the AI knows nothing, the best way to get good Q-Values is to try as many possibilities as possible (epsilon = 1), then I would slowly decrese it down to around 0.45, as the more it learns it becomes better to optimise the Q-Values by using them.\n",
    "\n",
    "\n",
    "(D) In the cell below, make it possible for us to produce from scratch a learning curve similar to Plot 1 but for a single agent, for a $k$ value of your own choosing. You do not need to include the baseline for random play.  This code should run in less than 30 seconds (ours runs in 2 seconds). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e65915a61d304027e4fbd2e714c4beba",
     "grade": true,
     "grade_id": "cell-e0e01e05236aee45",
     "locked": false,
     "points": 40,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import connect\n",
    "\n",
    "class connect3_Game():\n",
    "    \n",
    "    def __init__(self, VB = False):\n",
    "        self.env = connect.Connect(starting_player='x', verbose=VB)\n",
    "        self.state = self.env.grid\n",
    "        \n",
    "        self.agent = 'o'\n",
    "        self.opponent = 'x'\n",
    "        self.winner = False\n",
    "        self.totalMoves = 0\n",
    "        \n",
    "    def updateGame(self, reward, game_over):\n",
    "        self.totalMoves = self.totalMoves + 1\n",
    "        \n",
    "        if game_over:\n",
    "            self.winner = self.opponent\n",
    "            if reward == 1:\n",
    "                self.winner = self.agent\n",
    "        \n",
    "    def opponent_move(self):\n",
    "        '''\n",
    "            Has the opponent play in a random (available) location\n",
    "        '''\n",
    "        #print(\"Opponent Move\")\n",
    "        action = np.random.choice( self.env.available_actions )\n",
    "        reward, game_over = self.env.act( action )\n",
    "        self.updateGame(reward, game_over)\n",
    "        \n",
    "        return reward, action\n",
    "        \n",
    "    def agent_move(self, pos):\n",
    "        #print(\"Agent Move\")\n",
    "        reward, game_over = self.env.act(pos)\n",
    "        self.updateGame(reward, game_over)\n",
    "        return reward\n",
    "       \n",
    "        \n",
    "\n",
    "        \n",
    "class qLearning():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.qTable = dict()\n",
    "        \n",
    "        self.trueEpsilon = .7\n",
    "        self.epsilon = self.trueEpsilon\n",
    "        \n",
    "        self.alpha = .5\n",
    "        self.discountFactor = .7\n",
    "        self.currentEpisode = 0\n",
    "        self.lookAhead = 0 #How many actions to look forward when estimating max reward\n",
    "        self.newGame = connect3_Game(VB = False)\n",
    "        \n",
    "        self.agent = 'o'\n",
    "        self.opponent = 'x'\n",
    "        \n",
    "        \n",
    "        \n",
    "    def setupQValue(self, state):\n",
    "        if state in self.qTable:\n",
    "            return\n",
    "        \n",
    "        self.qTable[state] = np.zeros(5, dtype = float)\n",
    "        \n",
    "    def updateQValue(self, state, move, value):\n",
    "        self.setupQValue(state)\n",
    "        self.qTable[state][move] = value\n",
    "        \n",
    "    def getQValue(self, state, move):\n",
    "        self.setupQValue(state)\n",
    "        return self.qTable[state][move]\n",
    "    \n",
    "    \n",
    "    def getBestMove(self, state, game):\n",
    "        self.setupQValue(state)\n",
    "        best = -(10 ** 10)\n",
    "        usePos = 0\n",
    "        i = 0\n",
    "        for j in self.qTable[state]:\n",
    "            if j >= best and np.isin(i, game.env.available_actions):\n",
    "                best = j\n",
    "                usePos = i\n",
    "            \n",
    "            i = i + 1\n",
    "        \n",
    "        #print(\"Best Q Value: \", best)\n",
    "        return usePos\n",
    "        \n",
    "    \n",
    "    def hashGrid(self, game):\n",
    "        state = [0] * 15\n",
    "        i = 0\n",
    "        for row in game.env.grid:\n",
    "            for bit in row:\n",
    "                if bit == game.agent:\n",
    "                    state[i] = 1\n",
    "                elif bit == game.opponent:\n",
    "                    state[i] = -1\n",
    "                    \n",
    "                i = i + 1\n",
    "                \n",
    "        return tuple(state)\n",
    "\n",
    "        \n",
    "    \n",
    "    def eGreedy(self, game):\n",
    "        r = np.random.uniform()\n",
    "        \n",
    "        if r < self.epsilon or self.getBestMove( self.hashGrid(game), game ) == 0: #Choose a random move\n",
    "            return np.random.choice( game.env.available_actions )\n",
    "        else: #Choose best move\n",
    "            return self.getBestMove( self.hashGrid(game), game )\n",
    "        \n",
    "    def epsilonFactor(self, factor):\n",
    "        self.epsilon = self.minE + (self.maxE - self.minE) * factor\n",
    "        #print(\"epsilon: \", self.epsilon)\n",
    "        \n",
    "    \n",
    "    def getPossibleGames(self, game):\n",
    "        games = [None] * 5\n",
    "        for i in range(5):\n",
    "            if np.isin(i, game.env.available_actions):\n",
    "                tempGame = connect3_Game(VB = False)\n",
    "                tempGame.env.grid = np.copy( game.env.grid )\n",
    "                tempGame.env.active_player = game.env.active_player\n",
    "                tempGame.env.act(i)\n",
    "                \n",
    "                games[i] = tempGame \n",
    "                \n",
    "        return games\n",
    "                \n",
    "        \n",
    "    def estimatedReward(self, game, level):\n",
    "        trueMAX = -(10 ** 10)\n",
    "        MAX = trueMAX\n",
    "        \n",
    "        possibleGames = self.getPossibleGames(game)\n",
    "        i = 0\n",
    "        for GAME in possibleGames:\n",
    "            if GAME:\n",
    "                HASH = self.hashGrid(GAME)\n",
    "                qVal = self.getQValue(HASH, i)\n",
    "                if qVal > MAX:\n",
    "                    MAX = qVal\n",
    "                    \n",
    "            i = i + 1\n",
    "            \n",
    "        if MAX == trueMAX:\n",
    "            MAX = 0\n",
    "            \n",
    "        #if MAX != 0:\n",
    "            #print(\"Estimated: \", MAX)\n",
    "            \n",
    "        return MAX\n",
    "                \n",
    "        \n",
    "        \n",
    "    def rewardHandler(self, reward, game, state, action):\n",
    "        currentQ = self.getQValue(state, action)\n",
    "        \n",
    "        estimateMax = self.estimatedReward(game, 0)\n",
    "        #print(currentQ, reward, estimateMax)\n",
    "        qValue = (1-self.alpha) * currentQ + self.alpha * (reward + self.discountFactor * estimateMax)\n",
    "        #print(\"New Q value: \", qValue)\n",
    "        \n",
    "        #if currentQ != qValue:\n",
    "            #print(\"Old: \", currentQ, \" | New: \", qValue)\n",
    "                \n",
    "        self.updateQValue(state, action, qValue)\n",
    "\n",
    "    def setupGame(self):\n",
    "        self.newGame.env.reset()\n",
    "        self.newGame.env.active_player = self.opponent\n",
    "        self.newGame.winner = False\n",
    "        return self.newGame\n",
    "    \n",
    "    \n",
    "    def run_episode(self, showGrid):\n",
    "        newGame = self.setupGame()\n",
    "        self.currentEpisode = self.currentEpisode + 1\n",
    "        \n",
    "        moveRandom = False\n",
    "        newGame.env.act(2) #Starts the opponent in the middle\n",
    "        \n",
    "        while not newGame.winner:\n",
    "            \n",
    "            if moveRandom:\n",
    "                state = self.hashGrid(newGame)\n",
    "                reward, action = newGame.opponent_move()\n",
    "                self.rewardHandler(reward, newGame, state, action)\n",
    "                if newGame.winner:\n",
    "                    #if not showGrid and self.currentEpisode % 100 == 0:\n",
    "                        #print('Episode ', self.currentEpisode, ' finished, winner: ', newGame.winner)\n",
    "                    return newGame.winner\n",
    "            else:\n",
    "                moveRandom = True\n",
    "            \n",
    "            state, action = self.hashGrid(newGame), self.eGreedy(newGame) \n",
    "            reward = newGame.agent_move(action)\n",
    "            self.rewardHandler(reward, newGame, state, action)\n",
    "            if newGame.winner:\n",
    "                #if not showGrid and self.currentEpisode % 100 == 0:\n",
    "                    #print('Episode ', self.currentEpisode, ' finished, winner: ', newGame.winner)\n",
    "                return newGame.winner\n",
    "            \n",
    "    def learn(self, n):\n",
    "        self.epsilon = self.trueEpsilon\n",
    "        plays = 0\n",
    "        for i in range(n):\n",
    "            #game.epsilonFactor(1 - plays/maxN) #Starts learning by doing it randomly, then allows some use of Q-Values\n",
    "            game.run_episode(False)\n",
    "            \n",
    "            \n",
    "        #print(\"Finished learning (\", n, \" games)\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    def test(self, n):\n",
    "        self.epsilon = 0\n",
    "        \n",
    "        wins = 0\n",
    "        score = 0\n",
    "        for i in range(n):\n",
    "            winner = game.run_episode(False)\n",
    "            if winner == self.agent:\n",
    "                wins = wins + 1\n",
    "                score = score + 1\n",
    "            elif winner == self.opponent:\n",
    "                score = score - 1\n",
    "                \n",
    "        return wins, score\n",
    "    \n",
    "    def tester(self, n):\n",
    "        best = -1000\n",
    "        for j in range(1):\n",
    "            _, score = self.test(n)\n",
    "            if score > best:\n",
    "                best = score\n",
    "                \n",
    "        return best\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def rand_episode(self, showGrid):\n",
    "        newGame = self.setupGame()\n",
    "\n",
    "        while not newGame.winner:\n",
    "            \n",
    "            reward, action = newGame.opponent_move()\n",
    "            if newGame.winner:\n",
    "                return newGame.winner\n",
    "                \n",
    "            reward = newGame.agent_move( np.random.choice( newGame.env.available_actions ) )\n",
    "            if newGame.winner:\n",
    "                return newGame.winner       \n",
    "    \n",
    "    def rand_game(self, n):\n",
    "        wins = 0\n",
    "        score = 0\n",
    "        for i in range(n):\n",
    "            winner = game.rand_episode(False)\n",
    "            if winner == self.agent:\n",
    "                wins = wins + 1\n",
    "                score = score + 1\n",
    "            elif winner == self.opponent:\n",
    "                score = score - 1\n",
    "                \n",
    "        return wins, score        \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 500\n",
    "k = 8\n",
    "m = 10\n",
    "a = 1\n",
    "\n",
    "scores = np.zeros( (a, k) )\n",
    "for A in range(a): #Each Agent\n",
    "    \n",
    "    print('Agent ', A)\n",
    "    game = qLearning()\n",
    "    for K in range(k): #Learning Stages\n",
    "        game.learn(n)\n",
    "        \n",
    "        score = game.tester(m)\n",
    "        scores[A][K] = score\n",
    "        print('Score: ', score, ' (', K, '/', k,')')\n",
    "        \n",
    "averageScores = np.zeros(k)\n",
    "for C in range(k):\n",
    "    total = 0\n",
    "    for R in range(a):\n",
    "        total = total + scores[R][C]\n",
    "        \n",
    "    averageScores[C] = total / a\n",
    "    \n",
    "#print(scores)\n",
    "#print('')\n",
    "#print(averageScores)\n",
    "\n",
    "plt.plot(averageScores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('# of games learnt from')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT: How to submit.\n",
    "\n",
    "If any of the following instructions is not clear, please ask your tutors well ahead of the submission deadline.\n",
    "\n",
    "### Before you submit\n",
    "- We will not be able to mark your coursework if it takes more than 1 minutes to execute your entire notebook. That is, comment out (but do not delete) the code that you used to produce Plot 1 (i.e., learning curve averaged across many agents). Do **not** comment out the code that you use to produce a learning curve for a single agent (Exercise D).\n",
    "- Restart the kernel (_Kernel $\\rightarrow$ Restart & Run All_) and make sure that you can run all cells from top to bottom without any errors.\n",
    "- Make sure that your code is written in Python 3 (and not in Python 2!). You can check the Python version of the current session in the top-right corner below the Python logo.\n",
    "\n",
    "### Submission file\n",
    "- Please upload to Moodle a .zip file (**not** `.rar`, `.7z`, or any other archive format) that contains the completed Jupyter notebook (`ai4_connect_three.ipynb`) as well as the pre-computed figure(s). \n",
    "- **If** you change the `connect.py` file or write your own version of the environment, include the corresponding file in your submission, but give it any other name than `connect.py`. If you do not change its name, it will be overwritten  and we won't be able to execute your code! Make sure that you import the correct module when you rename your file, for example, use `import myConnect` if your file is called `myConnect.py`.\n",
    "- Do not include any identifying information. Not in the code cells, not in the file names, nowhere! Marking is anonymous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "game = qLearning()\n",
    "\n",
    "game.learn(2000)\n",
    "    \n",
    "testGames = 100\n",
    "wins, score = game.test(testGames)\n",
    "print(\"Score: \", score)\n",
    "print(\"Win rate: \", wins/testGames * 100)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
